{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 13 10:18:11 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.56       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:18:00.0 Off |                  N/A |\n",
      "| 29%   36C    P8    39W / 250W |   3830MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  On   | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 29%   30C    P8    21W / 250W |   2910MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 3090    On   | 00000000:AF:00.0 Off |                  N/A |\n",
      "|  0%   35C    P8    29W / 350W |   4157MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    271611      C   ...s/june2020-gpu/bin/python     3827MiB |\n",
      "|    1   N/A  N/A     51319      C   ...s/june2020-gpu/bin/python     2907MiB |\n",
      "|    2   N/A  N/A    291153      C   ...s/june2020-gpu/bin/python     1485MiB |\n",
      "|    2   N/A  N/A    299257      C   ...s/june2020-gpu/bin/python     2669MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Python 3 standard library\n",
    "from pathlib import Path\n",
    "\n",
    "from model.collectdata_kde_Ellipsoids import collect_t2kde_data\n",
    "from model.kde_loss_E import Loss\n",
    "from model.models_kde import TracksToKDE_Ellipsoids_DirtyDozen as Model\n",
    "from model.training_kde import trainNet, select_gpu, Results\n",
    "from model.plots import dual_train_plots, replace_in_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 available GPUs (initially using device 0):\n",
      "  0 GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "#The purpose of this Notebook is to compare different model's learning ability. Therefore, to even the playing field,\n",
    "#all models will learn at the same rate and for the same number of epochs. \n",
    "\n",
    "#Number of Learning Iterations\n",
    "n_epochs = 500\n",
    "#Learning Rate\n",
    "learning_rate = 1e-5\n",
    "#Size of Batches\n",
    "batch_size = 64\n",
    "\n",
    "architectures = [[5, 7, 10, 25, 50, 75, 100, 150, 250, 350, 500]]\n",
    "model_names = ['5to500_exp_inc_nodes']\n",
    "# architectures = [[5]*11,\n",
    "#                  [10]*11,\n",
    "#                  [15]*11\n",
    "#                  [20]*11\n",
    "#                  [25]*11\n",
    "#                  [30]*11\n",
    "#                  [5, 6, 8, 9, 11, 12, 14, 15, 17, 18, 20],\n",
    "#                  [5, 5, 5, 6, 7, 8, 10, 12, 14, 17, 20],\n",
    "#                  [5, 6, 7, 8, 9, 11, 12, 14, 16, 17, 20]]\n",
    "# model_names = ['5_nodes',\n",
    "#                '10_nodes',\n",
    "#                '15_nodes',\n",
    "#                '20_nodes',\n",
    "#                '25_nodes',\n",
    "#                '30_nodes',\n",
    "#                '5to20_lin_inc_nodes',\n",
    "#                '5to20_quadly_inc_nodes',\n",
    "#                '5to20_exp_inc_nodes']\n",
    "\n",
    "device = select_gpu(2)\n",
    "##device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ct, child =  0    Linear(in_features=9, out_features=5, bias=True)\n",
      "ct, child =  1    Linear(in_features=5, out_features=7, bias=True)\n",
      "ct, child =  2    Linear(in_features=7, out_features=10, bias=True)\n",
      "ct, child =  3    Linear(in_features=10, out_features=25, bias=True)\n",
      "ct, child =  4    Linear(in_features=25, out_features=50, bias=True)\n",
      "ct, child =  5    Linear(in_features=50, out_features=75, bias=True)\n",
      "ct, child =  6    Linear(in_features=75, out_features=100, bias=True)\n",
      "ct, child =  7    Linear(in_features=100, out_features=150, bias=True)\n",
      "ct, child =  8    Linear(in_features=150, out_features=250, bias=True)\n",
      "ct, child =  9    Linear(in_features=250, out_features=350, bias=True)\n",
      "ct, child =  10    Linear(in_features=350, out_features=500, bias=True)\n",
      "ct, child =  11    Linear(in_features=500, out_features=4000, bias=True)\n",
      "for model_dict\n",
      "index, k =   0    layer1.weight\n",
      "index, k =   1    layer1.bias\n",
      "index, k =   2    layer2.weight\n",
      "index, k =   3    layer2.bias\n",
      "index, k =   4    layer3.weight\n",
      "index, k =   5    layer3.bias\n",
      "index, k =   6    layer4.weight\n",
      "index, k =   7    layer4.bias\n",
      "index, k =   8    layer5.weight\n",
      "index, k =   9    layer5.bias\n",
      "index, k =   10    layer6.weight\n",
      "index, k =   11    layer6.bias\n",
      "index, k =   12    layer7.weight\n",
      "index, k =   13    layer7.bias\n",
      "index, k =   14    layer8.weight\n",
      "index, k =   15    layer8.bias\n",
      "index, k =   16    layer9.weight\n",
      "index, k =   17    layer9.bias\n",
      "index, k =   18    layer10.weight\n",
      "index, k =   19    layer10.bias\n",
      "index, k =   20    layer11.weight\n",
      "index, k =   21    layer11.bias\n",
      "index, k =   22    layer12.weight\n",
      "index, k =   23    layer12.bias\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2b9480a081aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m      \u001b[0;31m##print(\"model_dict instantiated\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# 1. filter out unnecessary keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mpretrained_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpretrained_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pretrained_dict iterated\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# 2. overwrite entries in the existing state dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_dict' is not defined"
     ]
    }
   ],
   "source": [
    "#Empty List of file destinations\n",
    "destinations = []\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    # Name is the output file name\n",
    "    name = time.strftime('%d%B%Y') + '_DirtyDozen_SetVar_' + model_names[i] + '_' + str(1000) + '_epochs_' + str(learning_rate)\n",
    "    \n",
    "    # Make an output folder named \"name\" (change if you want)\n",
    "    \n",
    "    # Special instructions for those working on goofy at UC\n",
    "    # Please be very careful to make sure that your folder\n",
    "    # does not live in a subdirectory of your home directory\n",
    "    # this disk has very little capacity. Instead, use \n",
    "    # a subdirectory in /share/lazy with a symbolic link to\n",
    "    # it in this (the notebooks) subdirectory\n",
    "    \n",
    "    folderpath = 'jgo_files/' + name\n",
    "    output = Path(folderpath)\n",
    "    output.mkdir(exist_ok=True)\n",
    "    \n",
    "    #Data Frame for storing learning results\n",
    "    results = pd.DataFrame([], columns=Results._fields)\n",
    "    \n",
    "    #Building the Model per settings written in 'architectures' array\n",
    "    nOut1, nOut2, nOut3, nOut4, nOut5, nOut6, nOut7, nOut8, nOut9, nOut10, nOut11 = architectures[i]\n",
    "    model = Model(nOut1,nOut2,nOut3,nOut4,nOut5,nOut6,nOut7,nOut8,nOut9,nOut10,nOut11)\n",
    "    \n",
    "    #Defining Loss Function\n",
    "    loss = Loss(epsilon=3e-6, debug=False)\n",
    "    \n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "      print('ct, child = ',ct, \"  \", child)\n",
    "      if ct < 0:\n",
    "        print(\"     About to set param.requires_grad=False for ct = \", ct, \"params\")\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False \n",
    "      ct += 1\n",
    "    \n",
    "    ##   ML -> /share/lazy/sokoloff/ML\n",
    "    model_dict = model.state_dict()\n",
    "    ## mds 190725 for debugging\n",
    "    print(\"for model_dict\")\n",
    "    index = 0\n",
    "    for k,v in model_dict.items():\n",
    "        print(\"index, k =  \",index,\"  \",k)\n",
    "        index = index+1\n",
    "    ##    print(\"value = \", v)\n",
    "\n",
    "    updated_dict = model_dict\n",
    "    ##print(\"updated_dict = \",updated_dict)\n",
    "    ## when starting \"ab initio\", reduce biases as the bias gets summed for each track\n",
    "    ## contributing to the predicted KDE\n",
    "    updated_dict[\"layer1.bias\"] = 0.005*model_dict[\"layer1.bias\"]\n",
    "    updated_dict[\"layer2.bias\"] = 0.005*model_dict[\"layer2.bias\"]\n",
    "    updated_dict[\"layer3.bias\"] = 0.005*model_dict[\"layer3.bias\"]\n",
    "    updated_dict[\"layer4.bias\"] = 0.005*model_dict[\"layer4.bias\"]\n",
    "    updated_dict[\"layer5.bias\"] = 0.005*model_dict[\"layer5.bias\"]\n",
    "    updated_dict[\"layer6.bias\"] = 0.005*model_dict[\"layer6.bias\"]\n",
    "    updated_dict[\"layer7.bias\"] = 0.005*model_dict[\"layer7.bias\"]\n",
    "    updated_dict[\"layer8.bias\"] = 0.005*model_dict[\"layer8.bias\"]\n",
    "    updated_dict[\"layer9.bias\"] = 0.005*model_dict[\"layer9.bias\"]\n",
    "    updated_dict[\"layer10.bias\"] = 0.005*model_dict[\"layer10.bias\"]\n",
    "    updated_dict[\"layer11.bias\"] = 0.005*model_dict[\"layer11.bias\"]\n",
    "    model.load_state_dict(updated_dict,strict=False)\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "#     olddate = '12July2021'\n",
    "#     previous = olddate + '_DirtyDozen_SetVar_' + model_names[i] + '_' + str(800) + '_epochs_' + str(learning_rate)\n",
    "#     pretrained_dict = torch.load('jgo_files/'+ previous + '/' + previous +'_final.pyt')\n",
    "#     print(\" \")\n",
    "#     print(\"  for pretrained_dict\")\n",
    "#     index = 0\n",
    "#     for k,v in pretrained_dict.items():\n",
    "#         print(\"index, k =  \",index,\"  \",k)\n",
    "#         index = index+1\n",
    "     ##print(\"model_dict instantiated\")\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    print(\"pretrained_dict iterated\")\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict) \n",
    "    ##\n",
    "    #   when starting from a model with a fully connected last layer rather than a convolutional layer\n",
    "    # 3. load the new state dict\n",
    "    #   need to use strict=False as the two models state model attributes do not agree exactly\n",
    "    #   see https://pytorch.org/docs/master/_modules/torch/nn/modules/module.html#Module.load_state_dict\n",
    "\n",
    "    model.load_state_dict(pretrained_dict,strict=False)\n",
    "\n",
    "    \n",
    "    #Transfering Model to GPU (or CPU, if chosen)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    #Defining Optimizer (Reminder to ask about that)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    # Data Import. If we are using a larger dataset (240K events, with the datasets above, and 11 GB  of GPU memory),\n",
    "    # the dataset will overflow the GPU memory; device=device will allow the data to move back\n",
    "    # and forth between the CPU and GPU memory. While this allows use of a larger dataset, it slows\n",
    "    # down performance by about 10%.  So comment out when not needed.\n",
    "    \n",
    "    # Defining training/validation partition point\n",
    "    slice_point = 18000\n",
    "    \n",
    "    #Training Data\n",
    "    train_loader = collect_t2kde_data('dataAA/20K_POCA_kernel_evts_200926.h5', batch_size=batch_size, device=device,\n",
    "                           slice = slice(None,slice_point))\n",
    "                            \n",
    "    # Validation dataset. You can slice to reduce the size.\n",
    "    val_loader = collect_t2kde_data('dataAA/20K_POCA_kernel_evts_200926.h5', batch_size=batch_size,                                device=device,\n",
    "                           slice = slice(slice_point,None))\n",
    "    \n",
    "    for result in trainNet(model, optimizer, loss, train_loader, val_loader, n_epochs, epoch_start=len(results), notebook=True):\n",
    "        \n",
    "        #Record Epoch Results\n",
    "        results = results.append(pd.Series(result._asdict()), ignore_index=True)\n",
    "\n",
    "        # Save each model state dictionary\n",
    "        torch.save(model.state_dict(), output / f'{name}_{result.epoch}.pyt')\n",
    "    \n",
    "    #Save final point in model\n",
    "    torch.save(model.state_dict(), output / f'{name}_final.pyt')\n",
    "    results.to_hdf(output / f'{name}_stats.hdf5', 'results')\n",
    "    destinations.append(output / f'{name}_stats.hdf5')\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print File destinations\n",
    "stringer = ''\n",
    "for file in destinations:\n",
    "    stringer = stringer + '\\'' + name + '\\', '\n",
    "print(stringer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = 'jgo_files/' + name\n",
    "output = Path(folderpath)\n",
    "torch.save(model.state_dict(), output / f'{name}_final.pyt')\n",
    "results.to_hdf(output / f'{name}_stats.hdf5', 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "june2020-gpu",
   "language": "python",
   "name": "june2020-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
